{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "from model import get_model\n",
    "from util import get_emd, emd_loss, grad_reverse, cos_sine\n",
    "from data import MultinomialBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "EPOCHS = 2000\n",
    "PLOT = True\n",
    "SAVE = True\n",
    "PLOT_EVERY = 10\n",
    "STEPS = EPOCHS // PLOT_EVERY\n",
    "\n",
    "N = 8\n",
    "Npoints = 6\n",
    "EMBED_DIM = 2\n",
    "scale = 4\n",
    "\n",
    "# here we define the parametrization of \n",
    "def p_from_paramterization(parameters, q=True):\n",
    "    t = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "    assert parameters.shape == (Npoints, 2)\n",
    "    \n",
    "    A, B, C, radii, centers, theta = parameters\n",
    "    # A, B, C = parameters\n",
    "    res = []\n",
    "    res.append(t * B + (1 - t) * A)\n",
    "    res.append(t * C + (1 - t) * B)\n",
    "    res.append(t * A + (1 - t) * C)\n",
    "    p = torch.vstack(res)\n",
    "    p = p.view(-1, EMBED_DIM)\n",
    "\n",
    "    theta = t * 2 * np.pi\n",
    "    ellipse = torch.cat([torch.cos(theta), torch.sin(theta)], dim=1) * radii /4 + centers\n",
    "    rot = theta[0] \n",
    "    ellipse = ellipse @ torch.tensor([[np.cos(rot), -np.sin(rot)], [np.sin(rot), np.cos(rot)]]).float()\n",
    "    return torch.cat([p, ellipse], dim=0)\n",
    "\n",
    "# Q\n",
    "parameters = torch.rand(Npoints, EMBED_DIM) * scale\n",
    "q = p_from_paramterization(parameters)\n",
    "q = q.view(-1, EMBED_DIM) + torch.randn_like(q) * 0.05\n",
    "E_q = torch.ones(q.shape[0], 1) / (q.shape[0])\n",
    "\n",
    "# P\n",
    "parameters = torch.rand(Npoints, EMBED_DIM) + scale / 2\n",
    "parameters = parameters.requires_grad_()\n",
    "\n",
    "\n",
    "p = p_from_paramterization(parameters).detach()\n",
    "E_p = torch.ones(p.shape[0], 1) / p.shape[0]\n",
    "\n",
    "def tensors_numpy():\n",
    "    # E_p = E_p_.detach().softmax(dim=0)\n",
    "    p = p_from_paramterization(parameters).detach()\n",
    "    return map(lambda x: x.detach().numpy(), [p, q, E_p.view(-1), E_q.view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emd_nn: +0.007| true: 0.089| Delta: -92.6%:  60%|██████████████████▍            | 1192/2000 [00:22<00:13, 58.77it/s]"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = get_model(\n",
    "    use_norm=True,\n",
    "    input_dim=EMBED_DIM,\n",
    "    latent_dim=128,\n",
    "    always_norm=False,\n",
    "    ngroups=32 // 2,\n",
    "    metric=2,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "optimizer_p = torch.optim.SGD((parameters, ), lr=2.5e-1, momentum=0.1, dampening=0.6)\n",
    "\n",
    "\n",
    "SCHEDULER = True\n",
    "if SCHEDULER:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=1e-5)\n",
    "\n",
    "\n",
    "E_scale = 1000\n",
    "p_numpy, q_numpy, E_p_numpy, E_q_numpy = tensors_numpy()\n",
    "yscale = 2 / E_p_numpy.max()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3.5), sharey=True)\n",
    "plt.tight_layout(pad=0.01, w_pad=0.01, h_pad=0.0)\n",
    "xsteps = 200\n",
    "ysteps = 250\n",
    "linspacex = torch.linspace(0, 1, xsteps) * scale\n",
    "linspacey = torch.linspace(0, 1, ysteps) * scale\n",
    "domain = torch.cartesian_prod(linspacex, linspacey)\n",
    "# vector field on domain\n",
    "subdomain = domain.view(xsteps, ysteps, 2)[::xsteps//10, ::ysteps//10].reshape(-1, 2)\n",
    "subdomain.requires_grad_()\n",
    "n = subdomain.shape[0]\n",
    "def init_ax(ax : plt.Axes):\n",
    "    heatmap = ax.imshow(\n",
    "        np.zeros((xsteps, ysteps)),\n",
    "        extent=(0, scale, 0, scale),\n",
    "        cmap=\"Blues_r\",\n",
    "        origin=\"lower\",\n",
    "        norm=LogNorm(vmin=0.01, vmax=1)\n",
    "    )\n",
    "\n",
    "    vfield = ax.quiver(\n",
    "        *subdomain.detach().T,\n",
    "        np.ones(n),\n",
    "        np.ones(n),\n",
    "        scale=.6,\n",
    "        scale_units=\"xy\",\n",
    "        color=\"black\",\n",
    "        alpha=0.6,\n",
    "        width=0.04,\n",
    "        units=\"xy\",\n",
    "    )\n",
    "    ax.set_xlim(0, scale)\n",
    "    ax.set_ylim(0, scale)\n",
    "\n",
    "    ax.set_yticks(np.arange(0, scale + 1))\n",
    "    ax.scatter(q_numpy[:, 0], q_numpy[:, 1], s=E_q_numpy * E_scale, c=\"forestgreen\")\n",
    "    colors = np.arange(Npoints)[ ... , None] + np.zeros(p_numpy.shape[0]//Npoints)\n",
    "    colors = colors.flatten() / (Npoints - 1)\n",
    "    sc = ax.scatter(\n",
    "        p_numpy[:, 0], p_numpy[:, 1], s=E_p_numpy * E_scale, c='crimson',\n",
    "    )\n",
    "    text = ax.text(0.0, 1.01, \"\", transform=ax.transAxes)\n",
    "    return vfield, heatmap, sc, text\n",
    "\n",
    "\n",
    "pbar = tqdm(total=EPOCHS, leave=True)\n",
    "\n",
    "\n",
    "def train_step(p, q, Ep=None, Eq=None):\n",
    "    fp = model(p)\n",
    "    fq = model(q)\n",
    "    return emd_loss(fp, fq, Ep, Eq)\n",
    "\n",
    "\n",
    "batcher = MultinomialBatch(p, q, E_p, E_q)\n",
    "\n",
    "epoch_count = 0\n",
    "def plot_step(i, vfield, heatmap, sc, text):\n",
    "    if DEBUG:\n",
    "        i = 1\n",
    "    epoch = epoch_count\n",
    "    for _ in range(i):\n",
    "        epoch +=  1\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_p.zero_grad()\n",
    "        # some stochastic steps\n",
    "        # for _ in range(1):\n",
    "        #     optimizer.zero_grad()\n",
    "        #     optimizer_p.zero_grad()\n",
    "        #     p_sample, q_sample = batcher(2)\n",
    "        #     loss = train_step(p_sample, q_sample)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     optimizer_p.zero_grad()\n",
    "        # optimizer_p.step()\n",
    "        p = p_from_paramterization(parameters)\n",
    "        # E_p = E_p_.softmax(dim=0)\n",
    "        loss = train_step(grad_reverse(p), q, grad_reverse(E_p), E_q)\n",
    "        # fp = model(p)\n",
    "        # fq = model(q)\n",
    "        # loss = (fp * E_p).sum() - (fq * E_q).sum()\n",
    "        emd_nn = -loss.item()\n",
    "        # loss += hkr_lambda * F.hinge_embedding_loss(\n",
    "        # torch.vstack([fp, fq]), - targets, margin=1)\n",
    "        # loss = F.cross_entropy(torch.vstack([fp, fq]), targets)\n",
    "        # loss = torch.log(1  + loss)\n",
    "        # loss = torch.log(loss + 1)\n",
    "        loss.backward()\n",
    "        # p.grad = - p.grad * (1 + torch.rand_like(p) * 0.01)\n",
    "        optimizer.step()\n",
    "        optimizer_p.step()\n",
    "        if SCHEDULER:\n",
    "            scheduler.step()\n",
    "        emd_true = get_emd(*tensors_numpy())\n",
    "        emd_diff = (emd_nn - emd_true) / emd_true * 100\n",
    "        msg = f\"emd_nn: {emd_nn:+.3f}| \"\n",
    "        msg += f\"true: {emd_true:.3f}| \"\n",
    "        msg += f\"Delta: {emd_diff:.1f}%\"\n",
    "        pbar.set_description(msg)\n",
    "        pbar.update()\n",
    "    p_numpy, q_numpy, E_p_numpy, E_q_numpy = tensors_numpy()\n",
    "    text.set_text(f\"epoch: {epoch} | emd:{emd_true:.2f}\")\n",
    "    output = model(subdomain)\n",
    "    grads = torch.autograd.grad(\n",
    "        output, subdomain, grad_outputs=torch.ones_like(output)\n",
    "    )[0]\n",
    "    with torch.no_grad():\n",
    "        output = model(domain).numpy().flatten()\n",
    "    output = output - output.min()\n",
    "    output/= 2\n",
    "    vfield.set_UVC(*grads.T)\n",
    "\n",
    "    c = cm.get_cmap(\"Blues\")(output.reshape(xsteps, ysteps))\n",
    "    heatmap.set_array(c)\n",
    "    sc.set_offsets(p_numpy)\n",
    "\n",
    "epochs = np.array([100, 300, 2500])\n",
    "for ax, step in zip(axes, epochs):\n",
    "    args = init_ax(ax)\n",
    "    plot_step(step, *args)\n",
    "fig.savefig(\"bothFit_test.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dda908cd4440e4a56212b851647e21ff71e3ab87d37174634fe81c6d0ddb9f62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
